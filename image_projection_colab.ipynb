{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dvschultz/make-ml-datasets/blob/master/image_projection_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_UqHzl-nrU5N"
   },
   "source": [
    "# Image Projection with PCA and UMAP\n",
    "\n",
    "Shared by Adam Evans ([repo](https://github.com/TheAdamEvans/bonsai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AcCvN34jYYDo"
   },
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4c_TadQzYL-k"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from umap import UMAP\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torch import no_grad, flatten\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SknqdSJtcETQ"
   },
   "source": [
    "Make a directory at the top level of Colab. This is where we will put our folder of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNdOpzV6a9fL"
   },
   "outputs": [],
   "source": [
    "!mkdir images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvIubNdkZmy4"
   },
   "source": [
    "## Get Dataset\n",
    "\n",
    "You have two options (I might recommend the second so you donâ€™t accidentally remove files from Drive):\n",
    "1. Work directly from Google Drive (first cell); or\n",
    "2. Download your images using gdown (second cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EPp0MPQIYeDo"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qNYaa_88Zz7i"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1nwzWHI8K_x7L9EePKVK3lrjLsqljakmC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX91EFnzjXru"
   },
   "outputs": [],
   "source": [
    "!unzip /content/annales1024.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSn9Rqg3aqwN"
   },
   "source": [
    "Once your images are accessible in Colab, move the folder of your images (the folder, not just the images) into the `images` folder at the top level in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d1E0X1ktYL-p"
   },
   "outputs": [],
   "source": [
    "path_to_image_folder = '/content/images/'\n",
    "# note there is a folder within this one with the jpg files\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AS_0pOcBYL-r"
   },
   "outputs": [],
   "source": [
    "# https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjY1L8wVYL-u"
   },
   "outputs": [],
   "source": [
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths\"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method that dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHMvKqxfYL-x"
   },
   "outputs": [],
   "source": [
    "def getImage(path, zoom):\n",
    "    return OffsetImage(plt.imread(path), zoom=zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw8T9f-NYL-0"
   },
   "outputs": [],
   "source": [
    "def plot_thumbs(embed, all_processed_image_paths, canvas_size=28, zoom=0.05):\n",
    "    '''Plot thumbnails at projected points in latent space'''\n",
    "    \n",
    "    assert X_features.shape[0] == len(all_processed_image_paths)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(canvas_size, canvas_size/1.618))\n",
    "    ax.scatter(embed[:,0], embed[:,1]) \n",
    "    \n",
    "    for x0, y0, path in zip(embed[:,0], embed[:,1], all_processed_image_paths):\n",
    "        ab = AnnotationBbox(getImage(path, zoom), (x0, y0), frameon=False)\n",
    "        ax.add_artist(ab)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxqevgJ4YL-3"
   },
   "outputs": [],
   "source": [
    "# imagenet preprocessing\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "# extension of ImageFolder\n",
    "# from https://gist.github.com/andrewjong/6b02ff237533b3b2c554701fb53d5c4d\n",
    "dataset = ImageFolderWithPaths(path_to_image_folder,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7kYxJYVYL-5"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIEBTcedYL-8"
   },
   "outputs": [],
   "source": [
    "all_processed_image_paths, all_features, all_images = [], [], []\n",
    "\n",
    "# iterate through batches and transform image into feature vector\n",
    "with no_grad():\n",
    "    for i, (images, target, paths) in enumerate(train_loader):\n",
    "        if i == 0: print('Converting images to features...')\n",
    "        \n",
    "        # run efficientnet over the images\n",
    "        # https://github.com/lukemelas/EfficientNet-PyTorch#example-feature-extraction\n",
    "        features = model.extract_features(images)\n",
    "        \n",
    "        flat_images = flatten(images, start_dim=1).detach().numpy()\n",
    "        flat_features = flatten(features, start_dim=1).detach().numpy()\n",
    "        \n",
    "        all_images.append(flat_images)        \n",
    "        all_features.append(flat_features)\n",
    "        all_processed_image_paths.extend(paths)\n",
    "        print(f'Featurized batch #{i} - {min(((i+1)*batch_size) / len(dataset),1.0)}')\n",
    "\n",
    "X_pixels = np.concatenate(all_images)\n",
    "X_features = np.concatenate(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcyB8H_PYL--"
   },
   "outputs": [],
   "source": [
    "X_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jt1iJ8xKYL_B"
   },
   "outputs": [],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZJsVpA5gYL_D"
   },
   "outputs": [],
   "source": [
    "# reduce raw channels X pixels to 2 dimensions with PCA\n",
    "pca = PCA(n_components=2)\n",
    "embed = pca.fit_transform(X_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ven0lQOGYL_F"
   },
   "outputs": [],
   "source": [
    "# lovely dark --> light embedding\n",
    "plot_thumbs(embed, all_processed_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erCCY-FPYL_H"
   },
   "outputs": [],
   "source": [
    "# reduce raw channels X pixels to 2 dimensions with UMAP\n",
    "umap = UMAP(n_components=2)\n",
    "embed = umap.fit_transform(X_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6nNWd-zOYL_K"
   },
   "outputs": [],
   "source": [
    "# more.. compact? dark --> light embedding\n",
    "plot_thumbs(embed, all_processed_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ek0ptWr-YL_N"
   },
   "outputs": [],
   "source": [
    "# reduce features from efficientnet to 2 dimensions with PCA\n",
    "pca = PCA(n_components=2)\n",
    "embed = pca.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4UNpYmVYL_P"
   },
   "outputs": [],
   "source": [
    "# seems to pick up a more triangular pattern\n",
    "# direction is much easier to interpret\n",
    "#   --> goes from busy image / bushy to the left and clean / isolated to the right\n",
    "plot_thumbs(embed, all_processed_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GWFqVn-9YL_R"
   },
   "outputs": [],
   "source": [
    "# reduce features from efficientnet to 2 dimensions with UMAP\n",
    "umap_2 = UMAP(n_components=2)\n",
    "embed = umap_2.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6XjNnDPYL_U"
   },
   "outputs": [],
   "source": [
    "# some kind of embedding.. seems to pick up on same-scene \"duplicates\" nicely\n",
    "plot_thumbs(embed, all_processed_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels or content? which one is right? why not both!\n",
    "content_weight = 2\n",
    "pixel_weight = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the column values\n",
    "features_scaler = StandardScaler()\n",
    "pixels_scaler = StandardScaler()\n",
    "X_features_nml = features_scaler.fit_transform(X_features)\n",
    "X_pixels_nml = pixels_scaler.fit_transform(X_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pixels data has *more* columns so needs to be downweighted proportionally\n",
    "# calculate these in order to make pixels & content have equal weight\n",
    "total_column_width = X_pixels_nml.shape[1] + X_features_nml.shape[1]\n",
    "pixel_column_width = X_pixels_nml.shape[1]\n",
    "features_column_width = X_features_nml.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([\n",
    "    pixel_weight*(total_column_width/pixel_column_width)*X_pixels_nml,\n",
    "    content_weight*(total_column_width/features_column_width)*X_features_nml,\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce concatenated pixel+features matricies to 2 dimensions with PCA\n",
    "reducer = UMAP(n_components=2)\n",
    "embed = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_thumbs(embed, all_processed_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uv7R6KYtYL_W"
   },
   "outputs": [],
   "source": [
    "rm -r /content/images/bonnebone-256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xcEiiXsfke8x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "image_projection_colab.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
